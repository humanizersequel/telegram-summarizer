Okay, here's a README file for the script.

# Telegram Chat Insights & Report Generator

This Python script processes exported Telegram chat logs (in JSON format) to extract key insights using Anthropic's Claude API and generate a comprehensive, readable report. It's designed to handle long conversations by processing messages in chunks, maintaining a rolling summary of insights, and saving its progress, allowing for incremental updates.


## Prerequisites

1.  **Python 3.x:** Ensure you have Python 3 installed (preferably 3.8 or newer).
2.  **Anthropic API Key:** You need an API key from Anthropic to use Claude models.
3.  **Required Python Libraries:** Install the necessary libraries using pip:
    ```bash
    pip install anthropic tiktoken
    ```
    * `anthropic`: The official Python client for the Anthropic API.
    * `tiktoken`: Used for accurately counting tokens to manage LLM context windows.

## Setup

### 1. Configure the API Key

You **must** replace the placeholder API key in the script with your own Anthropic API key.

Open the script (e.g., `telegram_summarizer.py`) in a text editor and find this line near the top:

```
# HARDCODE YOUR API KEY HERE (NOT RECOMMENDED FOR PRODUCTION/SHARED CODE)
ANTHROPIC_API_KEY = "YOUR API KEY HERE" # Replace with your actual key
```

Replace the dummy variable with your actual API key.

### 2. Directory Structure

The script operates on a specific directory structure:

1.  Create a **main working directory** where you will place the Python script (e.g., `telegram_analysis`).
2.  Inside this main directory, create a **subdirectory for each Telegram chat** you want to analyze. The name of this subdirectory will be used in naming the state file and the final report (e.g., `project_alpha_chat`).
    ```
    telegram_analysis/
    ├── telegram_summarizer.py  (This script)
    ├── project_alpha_chat/
        └── result_export_1.json  (Telegram export file for this chat)
        └── another_export_for_alpha.json (Optional: newer export)
        └── project_alpha_chat-state.json (Generated by the script)
        └── project_alpha_chat-Report-YYYY-MM-DD_HHMMSS.txt (Generated by script)
    ```

### 3. Getting Telegram Chat Exports (JSON Format)

This script relies on JSON exports from the **Telegram Desktop application**.

* **Important Distinction:** You need the version of "Telegram Desktop" typically downloaded directly from the [Telegram website](https://desktop.telegram.org/), not necessarily the version from app stores (like the Mac App Store's "Telegram for macOS," which has historically had different features). The cross-platform "Telegram Desktop" client is more likely to have the full export functionality.
* **How to Export (General Steps for Telegram Desktop):**
    1.  Open Telegram Desktop.
    2.  Select the chat (private, group, or supergroup) you want to export.
    3.  In the chat view, click the **three-dot menu (⋮)**, usually in the top-right corner of the chat panel.
    4.  Select "**Export chat history...**".
    5.  In the "Chat export settings" dialog:
        * **Format:** Choose **"Machine-readable JSON"**. This is critical.
        * **Content:** Select what you want (text messages are essential). You can choose to include or exclude media. If media files are large and not included in the export directly, the JSON will contain placeholders like `(File not included...)`. The script primarily processes text.
        * **Size Limit:** Adjust if necessary.
        * **Path:** Choose a location to save the export. It will typically create a folder containing the `result.json` file (or similar) and media subfolders if media was included.
    6.  Click "Export".
        * **First-time export:** Telegram might impose a 24-hour security delay. You might need to confirm from another device or wait.
    7.  Once exported, copy the main JSON file (e.g., `result.json`) into the appropriate chat subdirectory you created in Step 2.

* **Troubleshooting Export:**
    * If the export option is missing, ensure you're using the correct Telegram Desktop version (from desktop.telegram.org).
    * For groups with "Topics," try switching the view to "View as Messages" to see if the export option appears for the whole group.
    * Secret Chats cannot be exported.

## Running the Script

1.  Open your terminal or command prompt.
2.  Navigate to the main working directory where you saved `telegram_summarizer.py`.
3.  Run the script:
    ```bash
    python telegram_summarizer.py
    ```
4.  The script will prompt you:
    ```
    Enter the name of the chat folder to process (e.g., 'project_alpha_chat'):
    ```
5.  Enter the name of the chat subdirectory (e.g., `project_alpha_chat`) and press Enter.

The script will then:
* Look for a `yourfoldername-state.json` file in that chat folder. If found, it loads the last processed state.
* Find the newest JSON export file in the chat folder.
* Process new messages in chunks, interacting with the Anthropic API.
* Save individual chunk insights and the last processed message ID to the state file after each chunk.
* If all new messages are processed successfully, it will generate (or regenerate) a final report as a `.txt` file in the chat folder.
* Delete the processed JSON export file if all steps (including report generation, if warranted) were successful.

**Subsequent Runs:**
Simply place a newer export JSON file into the same chat folder (you can configure Telegram to save them directly to the target directory) and re-run the script, providing the same folder name. It will pick up where it left off.

## Customizing Prompts

The script contains several system prompts that instruct the LLM on how to behave for different tasks. You can customize these to tailor the output to your needs. They are located near the top of the script:

* **`SYSTEM_PROMPT_CHUNK_INSIGHTS`**: Defines how the LLM should extract insights from individual message chunks, given the rolling summary of previous insights. You can change this to focus on different aspects (e.g., sentiment, specific keywords, more or less detail).
* **`SYSTEM_PROMPT_COMPACT_INSIGHTS_COLLECTION`**: Used when the rolling summary of previous insights (or the collection of all insights before final report generation) becomes too long and needs to be condensed. You can adjust this to control how aggressively it summarizes.
* **`SYSTEM_PROMPT_FINAL_REPORT`**: Dictates the structure, format, and content focus of the final `.txt` report. This is where you can specify if you want more narrative, different sections, or a particular style.

**To customize:**
1.  Open the `telegram_summarizer.py` file.
2.  Locate these string variables.
3.  Modify the text within the triple quotes (`"""..."""`) to change the instructions.
4.  Save the file. The next time the script runs and uses that particular prompt, it will use your updated version.

The state file also records which set of prompts were used when its contents were generated, which can be helpful for tracking changes if you experiment with prompts over time.

## Modifying Processing Parameters

Near the top of the script, you can also adjust:

* **`MESSAGES_PER_CHUNK`**: How many messages are processed by the LLM in one go. Smaller values mean more API calls but more frequent saving of progress and potentially more granular insights. Larger values mean fewer API calls but longer processing per chunk.
* **`MAX_PREVIOUS_INSIGHTS_TOKENS_FOR_CHUNK_PROCESSING`**: The token limit for the rolling summary context provided to the LLM when processing a new chunk. If the rolling summary exceeds this, it will be compacted.
* **`MAX_TOKENS_FOR_INSIGHT_COLLECTION_COMPACTION`**: The maximum number of tokens the script will attempt to send to the LLM *for the specific task of compacting a list of insights*.
* **`MAX_INSIGHTS_PER_META_CHUNK_FOR_REPORT`**: If the total list of individual insights is very long, this controls how many insights are grouped together for an intermediate summarization step before the final report is generated.
* **`MAX_TOKENS_FOR_FINAL_REPORT_GENERATION`**: The token limit for the (potentially multi-stage summarized) input that goes into generating the final textual report.
* **Retry Parameters (`MAX_API_RETRIES`, `INITIAL_BACKOFF_SECONDS`, `MAX_BACKOFF_SECONDS`):** Control the API call retry behavior.

Adjust these based on the length and density of your chats, desired output detail, and your budget for API calls.

## Troubleshooting

* **`ANTHROPIC_API_KEY not configured`**: Ensure you've replaced the placeholder API key in the script.
* **`UnboundLocalError` or `KeyError`**: Could indicate an unexpected structure in your JSON export or a bug in how the script parses it. Providing a snippet of the problematic JSON can help debug.
* **API Errors (e.g., 529 Overloaded, 429 Rate Limit)**: The script has retry logic. If these persist, the API provider might be experiencing ongoing issues, or you might be hitting usage limits on your API account.
* **Token Limit Errors from API (e.g., 400 Bad Request mentioning tokens)**:
    * The script tries to manage this with compaction, but if a single message chunk or a compacted summary is still too large, you might need to:
        * Reduce `MESSAGES_PER_CHUNK`.
        * Make the compaction prompts more aggressive in demanding shorter outputs.
        * Adjust the `MAX_..._TOKENS_...` configuration values.
* **Report/Insights Not Detailed Enough**: Experiment with and refine the system prompts, especially `SYSTEM_PROMPT_FINAL_REPORT` and `SYSTEM_PROMPT_CHUNK_INSIGHTS`, to ask for more detail or specific types of information. You might also consider reducing `MESSAGES_PER_CHUNK` to get more granular initial insights.

Good luck!
```
